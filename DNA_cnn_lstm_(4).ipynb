{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmQPnpvpkuid"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVWe2sZ4lZFc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/human.txt' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52K_R7vylZL4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7ZqSdYTlZPm"
      },
      "outputs": [],
      "source": [
        "# split column into multiple columns by delimiter \n",
        "df[['sequence','class']] = df['sequence\\tclass'].str.split('\\t', expand = True)\n",
        "human_df = df.drop('sequence\\tclass' ,  axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp54NVQ8lZSl"
      },
      "outputs": [],
      "source": [
        "human_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3urXFUWqvYn"
      },
      "outputs": [],
      "source": [
        "human_df['class'].value_counts().sort_index().plot.bar()\n",
        "plt.title(\"Class distribution of human DNA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBEa5PCglZVo"
      },
      "outputs": [],
      "source": [
        "def Kmers_funct(seq, size):\n",
        "    s = [seq[x:x+size].lower() for x in range(0 , len(seq) - size + 1 ,6)]\n",
        "    return ' '.join(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc3Ow82hLZvs"
      },
      "source": [
        "k-mers function returns all of a sequence's subsequences of length , such that the sequence AGAT would have four monomers (A, G, A, and T), three 2-mers (AG, GA, AT), two 3-mers (AGA and GAT) and one 4-mer (AGAT) .\n",
        "i've converted the dna sequence to a sentece of k mers in order to treat it as a normal nlp task ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU7MnKNhlZYY"
      },
      "outputs": [],
      "source": [
        "human_df['k_mers'] = human_df['sequence'].apply((lambda x: Kmers_funct(x , 9)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA3RzXVblZb0"
      },
      "outputs": [],
      "source": [
        "human_df = human_df.drop('sequence' , axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OleYJUNmOri"
      },
      "outputs": [],
      "source": [
        "human_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyuu7S82L_7Y"
      },
      "source": [
        "# Tokenization ‚úÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdl6VelQmOum"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "max_features = 90000\n",
        "tk = Tokenizer(lower = True , num_words = max_features)\n",
        "full_text = list(human_df['k_mers'].values)\n",
        "tk.fit_on_texts(full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5roWIC0mOxb"
      },
      "outputs": [],
      "source": [
        "human_df['t'] = tk.texts_to_sequences(human_df['k_mers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3r6IrYamO0T",
        "outputId": "f25de48e-0fb5-4a2c-d424-9d41d27b94aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2472"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "m = np.max(human_df['t'].apply((lambda x: len(x))))\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_QBKL9EmO3I",
        "outputId": "ee5d597b-14c7-468b-ca71-aabfdf0da6dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4380, 2472)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "max_len = 2472\n",
        "x = pad_sequences(human_df['t'], maxlen = max_len)\n",
        "x.shape\n",
        "#adding a pad to our tokenized sentence to make them all of the same length ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMD5b7p1mO54",
        "outputId": "9fbebae8-fb2b-4a62-f1af-ceb91cc7fbb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4380,)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "y = np.array(human_df['class'])\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GX7MGWJmO9P"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "adasyn = ADASYN()\n",
        "x, y = adasyn.fit_resample(x , y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lzEn23yTXkfm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7C3opA7M5dT"
      },
      "source": [
        "**SMOTE üéà:** \n",
        "\n",
        "\n",
        "\n",
        "Synthetic Minority Over sampling Technique (SMOTE) algorithm applies KNN approach where it selects K nearest neighbors, joins them and creates the synthetic samples in the space. The algorithm takes the feature vectors and its nearest neighbors, computes the distance between these vectors. The difference is multiplied by random number between (0, 1) and it is added back to feature. SMOTE algorithm is a pioneer algorithm and many other algorithms are derived from SMOTE , one of these algorithms is ADASYN .\n",
        "\n",
        "üçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇ\n",
        "\n",
        "**ADASYN ‚ùÑ:**\n",
        "\n",
        "ADAptive SYNthetic (ADASYN) is based on the idea of adaptively generating minority data samples according to their distributions using K nearest neighbor. The algorithm adaptively updates the distribution and there are no assumptions made for the underlying distribution of the data.  The algorithm uses Euclidean distance for KNN Algorithm.\n",
        "\n",
        "üçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇüçÇ\n",
        "\n",
        "**The key difference between ADASYN and SMOTE üåª :**\n",
        "\n",
        " is that the former dont put the generated elements in linearity with the original ones , instead a random small number will be added to break the linear position , this render our generated instances more realistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFTH0qY5KUxY",
        "outputId": "64d6902d-3707-4a76-e0e9-171dd35e99a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['0' 1276]\n",
            " ['1' 1371]\n",
            " ['2' 1346]\n",
            " ['3' 1239]\n",
            " ['4' 1334]\n",
            " ['5' 1348]\n",
            " ['6' 1343]]\n"
          ]
        }
      ],
      "source": [
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yil301P7ObKV"
      },
      "source": [
        "now we can see that the data is balanced "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN_LSTM :\n",
        "i've used the cnn conv1D layers for feature extraction from text since it is hard for lstm to detect features , beside the impossible fact of select ing features manually as we were used to do in data mining problems , so cnn was the best choice then i've used lstm since it have a long term memory for both direction wich maybe important in our case to help in dna classification ."
      ],
      "metadata": {
        "id": "C0ZlYQso1Rhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw1FITEkmdec"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(Embedding(90000 , 1000 , input_length = 2472))\n",
        "model.add(Conv1D(filters = 128 , kernel_size = 5, padding = 'valid', activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = 2))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(Conv1D(filters = 256 , kernel_size = 3, padding = 'valid', activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(LSTM(40, dropout=0.4, recurrent_dropout=0.5 , return_sequences= True))\n",
        "model.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2 , return_sequences= False))\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCQLC5VUmdhO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x , y , test_size = 0.30, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtnVxASyn9W9"
      },
      "outputs": [],
      "source": [
        "y_train = [np.int32(numeric_string) for numeric_string in y_train]\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp_D_iDNJ1PU"
      },
      "outputs": [],
      "source": [
        "y_test = [np.int32(numeric_string) for numeric_string in y_test]\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYjwqYJMmdkF"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, epochs = 5 , batch_size = 200 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs0rYO38zigW"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test , y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMD0xo0QmdqC"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "# from keras.models import Sequential\n",
        "# from tensorflow.compat.v1.keras.layers import Concatenate\n",
        "# cnn_model = Sequential()\n",
        "# cnn_model.add(Embedding(9000, 100, input_length = 649))\n",
        "# cnn_model.add(Conv1D(filters = 128, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
        "# cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# cnn_model.add(Conv1D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
        "# cnn_model.add(MaxPooling1D(pool_size = 2))\n",
        "\n",
        "# cnn_model.add(Conv1D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
        "# cnn_model.add(MaxPooling1D(pool_size = 2))\n",
        "# cnn_model.add(Flatten())\n",
        "\n",
        "# lstm_model = Sequential()\n",
        "# lstm_model.add(Embedding(9000, 100, input_length = 649))\n",
        "# lstm_model.add(Bidirectional(CuDNNLSTM(50, return_sequences = True)))\n",
        "# lstm_model.add(Flatten()) #we can delete flatten if we set return sequences to false \n",
        "\n",
        "# merge = Concatenate([lstm_model, cnn_model])\n",
        "# hidden = Dense(7, activation = 'sigmoid')\n",
        "# conc_model = Sequential()\n",
        "# conc_model.add(merge)\n",
        "# conc_model.add(hidden)\n",
        "# c_model = Model([(649,1),(649,1)],1)\n",
        "# Input(shape=(27, 27, 1))\n",
        "# conc_model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9CHuU_R3sLJ"
      },
      "outputs": [],
      "source": [
        "# print(c_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x00LNsCl5S2w"
      },
      "outputs": [],
      "source": [
        "# conc_model.fit(x_train, y_train, epochs = 20, batch_size = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxw1MuhgoTk4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EICtuRC83xly"
      },
      "outputs": [],
      "source": [
        "# from keras.layers.merge import concatenate\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Input , MaxPooling1D\n",
        "\n",
        "# model1_in = Input(shape=(649))\n",
        "# model11_out = Embedding(9000, 300)(model1_in)\n",
        "# model12_out = Conv1D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')(model11_out)\n",
        "# model13_out = MaxPooling1D(pool_size=2)(model12_out)\n",
        "# model1_out = Flatten()(model13_out)\n",
        "# model1 = Model(model1_in, model1_out)\n",
        "\n",
        "# model2_in = Input(shape=(649))\n",
        "# model21_out = Embedding(9000, 300)(model2_in)\n",
        "# model22_out = Bidirectional(LSTM(20, return_sequences = True))(model21_out)\n",
        "# model2_out = Flatten()(model22_out)\n",
        "# model2 = Model(model2_in, model2_out)\n",
        "\n",
        "\n",
        "# concatenated = concatenate([model1_out, model2_out])\n",
        "# out = Dense(1, activation='softmax', name='output_layer')(concatenated)\n",
        "\n",
        "# merged_model = Model([model1_in, model2_in], out)\n",
        "# merged_model.compile(loss='binary_crossentropy', optimizer='adam', \n",
        "# metrics=['accuracy'])\n",
        "\n",
        "# checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc',\n",
        "# save_best_only=True, verbose=2)\n",
        "# early_stopping = EarlyStopping(monitor=\"val_loss\", patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3usaccrCTO8H"
      },
      "outputs": [],
      "source": [
        "# merged_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q23Q1yKuTMpH"
      },
      "outputs": [],
      "source": [
        "# merged_model.fit([x_train , x_train ], y = y_train, batch_size = 100, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EClnGnDhLof"
      },
      "outputs": [],
      "source": [
        "# merged_model.evaluate([x_train , x_train ], y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbo-9c7pm12q"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "# from keras.models import Sequential\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(9000 , 100 , input_length = 649))\n",
        "# model.add(Conv1D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(SpatialDropout1D(0.4))\n",
        "# model.add(Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Conv1D(filters = 128, kernel_size = 5, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(SpatialDropout1D(0.4))\n",
        "# model.add(Bidirectional(LSTM(40, dropout=0.5, recurrent_dropout=0.2 , return_sequences= False)))\n",
        "# model.add(Dense(10,activation='ReLU'))\n",
        "# model.add(Dense(7,activation='softmax'))\n",
        "# model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUC18YxcnVci"
      },
      "outputs": [],
      "source": [
        "# model.fit(x_train, y_train, epochs = 10, batch_size = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaAdkRQnoUva"
      },
      "outputs": [],
      "source": [
        "# model.evaluate(x_test ,y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}